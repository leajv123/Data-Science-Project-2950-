{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phase IV: Final Results Draft\n",
    "\n",
    "#### by Katherine Vella (skv25), Fatima Yuen (fy46), and Lea Jih-Vieira (laj74)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Contents:\n",
    "I. Introduction </br>\n",
    "II. Data Description </br>\n",
    "III. Preregistration Statement </br>\n",
    "IV. Data Analyses </br>\n",
    "V. Evaluation of Significance </br>\n",
    "VI. Interpretations and Conclusions </br>\n",
    "VII. Limitations <br>\n",
    "VIII. Source Code </br>\n",
    "IX. Acknowledgements </br>\n",
    "X. Appendix: Data Cleaning Description </br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The History of Stop-and-Frisk in NYC:\n",
    "\n",
    "The New York Police Department’s stop-and-frisk program is a crime-prevention strategy in which officers temporarily detain, question, and often search individuals suspected of carrying weapons or contraband. The program gained traction in the early 2000s under Mayor Michael Bloomberg and reached its height in 2011, a year in which an estimated 685,724 civilians were stopped by the NYPD. In August of 2013, US District Court Judge Sheila Scheindlin ruled in Floyd v. City of New York that stop-and-frisk had been implemented by the NYPD in an unconstitutional manner. This decision marked a turning point in the program through the subsequent NYPD mandate that all officers thoroughly justify and document the reason for each stop. Instances of stop-and-frisk declined in the following years due to accusations of racial profiling, as many believed the program disproportionately targeted African-American and Latino individuals. While the percentage of NYPD resources that was devoted to the stop-and-frisk program is not publicly available, the funding and manpower that must have been required to make hundreds of thousands of stops for over a decade begs the question: was the NYPD’s stop-and-frisk program truly effective at reducing crime in New York? Or, was it simply an act of racial discrimination designed to incite fear in the streets? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Question(s):\n",
    "\n",
    "We are interested in evaluating crime in New York City over a 10 year period. How has crime changed in New York City from 2009 to 2019? How has the prevalence of certain crimes transformed over this timespan?\n",
    "\n",
    "We are also interested in how implementation of certain policing programs has potentially influenced the crime statistics we find. In particular, we are interested in how the Stop, Question, and Frisk program influenced crime statistics during this period.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset #1: Stop, Question, and Frisk (2011)\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "This dataset was created to document every police-civilian encounter that falls under the NYPD Stop, Question, and Frisk program. The New York City Police Department (NYPD) created the dataset for their records; it is also available for public use. The dataset was funded by the government via taxpayer dollars.\n",
    "\n",
    "**Composition**\n",
    "\n",
    "Each instance (row) of this dataset is a recorded stop of a civilian during 2011. After removing irrelevant columns of data, the dimensions of this dataset was 685,724 x 23. 2011 was the height of the Stop, Question, and Frisk program, which provides an explanation as to why this dataset is so large. We then took a random sample of 10% of this data.\n",
    "\n",
    "Each column represents a different datum regarding the stop recorded for that row. We kept a total of 23 data points regarding a single stop. Those 23 points were about a suspect’s race, age, sex, if an arrest was made, how long they were observed for, how long they were stopped for, the borough the stop was made in, whether a weapon was found on them, if the officer used physical force, and whether a summons was issued to them.\n",
    "\n",
    "This dataset contains all possible instances of people who were stopped during the Stop, Question, and Frisk program. There is the chance that some stops were not recorded in this dataset for reasons outside of our control (ex: an officer did not report it), but if the dataset stays true to its original purpose, it should contain every instance of a stop during the Stop, Question, and Frisk program in 2011.\n",
    "\n",
    "Each instance (row) of this dataset is a recorded stop of a civilian during 2011. Each instance consists of information on the suspect’s race, age, sex, if an arrest was made, how long they were observed for, how long they were stopped for, the borough the stop was made in, whether a weapon was found on them, if the officer used physical force, and whether a summons was issued to them.\n",
    "\n",
    "Each row is assigned an index, starting from count 0. These indices can be used to identify each instance. \n",
    "\n",
    "There are null inputs for some columns but we can generally assume that null means none or no. For example, if no arrest was made, the input for an arrest offense may be null and in this case, null means there is no arrest offense. We are limited in our understanding of what these null inputs mean, as the data specifications released by the NYPD do not address the null entries. Given what we know, it is our most educated guess that the null inputs mean none or no. Relationships between individual instances are not made explicit. One column does record if another person(s) present at the scene of the encounter were also searched, but does not indicate the specific person(s), if any.\n",
    "\n",
    "There are no recommended data splits provided. There are no errors or redundancies in the dataset itself. In the raw dataset, there were definitely values that could be interpreted as redundant, but those were dealt with during the cleaning process. This dataset is self-contained. It does not link to external sources or other datasets.\n",
    "\n",
    "There does not seem to be any confidential data. In addition, for those who are arrested for possession of illegal items, it goes on their record which is a public record. There are no identifying markers of any instances.\n",
    "\n",
    "The dataset may cause some anxiety for those who have lived experience with the Stop, Question, and Frisk program, or those who have negative feelings regarding policing, etc. This dataset does relate to people. This dataset records those who are stopped by police on suspicion of possessing illegal firearms, knives, contraband, etc. It should not be possible to identify individuals from this data after our cleaning process.\n",
    "\n",
    "**Collection Process**\n",
    "\n",
    "The data was collected through the police reports written after a police stop of a civilian was made. This dataset relied on police officers to report and record instances to collect the data. The NYPD was involved in the data collection process. This data was collected in part for the Stop, Question, and Frisk Database that the NYPD maintains. The timeframe of this dataset was the year 2011. No ethical review processes were conducted to our knowledge. Each instance of this dataset represents a person.\n",
    "\n",
    "\n",
    "**Pre-Processing / Cleaning / Labeling**\n",
    "\n",
    "There was no pre-processing done to the data prior to our data cleaning. Our data cleaning included dropping unecessary and empty columns, renaming / reorganizing columns, dropping instances with ages outside our decided age range, dropping instances with an empty boro column, dropping instances with an observation duration of 999, recoding race data, and converting all columns eligible to be represented by True/False values to those boolean values. Then, a 10% random sample was drawn from this population data. A much more detailed account of the data cleaning process is available in the Section X. Appendix: Data Cleaning Description.\n",
    "\n",
    "**Uses**\n",
    "\n",
    "We are unaware if this data has been used for any tasks already. This dataset could definitely be used for an in-depth analysis of the Stop, Question, and Frisk program as a whole, or an analysis of different NYPD-implemented policing programs. I am unaware of any tasks this data should not be used for; if the task is relevant to the Stop, Question, and Frisk Program, this dataset is definitely abundant enough to be used for it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset #2: Stop, Question, and Frisk (2019)\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "The dataset was created to record the stops made under New York City Police Department’s Stop, Question, and Frisk program in 2019. The dataset was created by the NYPD. The dataset is funded by taxpayer dollars.\n",
    "\n",
    "**Composition**\n",
    "\n",
    "The instances that comprise the dataset represent every person that was stopped under the Stop, Question, and Frisk program. There are 13,460 instances, meaning 13,460 people were stopped in 2019. The dataset contains all instances of stops in the year 2019. Each instance consists of data on the person’s race, age, sex, if an arrest was made, how long they were observed for, how long they were stopped for, the borough the stop was made in, whether a weapon was found on them, if the officer used physical force, and whether a summons was issued to them. The label associated with each instance is the index of the row that represents that instance.\n",
    "\n",
    "There are null inputs for some columns but we can generally assume that null means none or no. For example, if no arrest was made, the input for an arrest offense may be null and in this case, null means there is no arrest offense. We are limited in our understanding of what these null inputs mean, as the data specifications released by the NYPD do not address the null entries. Given what we know, it is our most educated guess that the null inputs mean none or no. No, they are not made explicit. One of the observations for each instance is if another person was also stopped along with them but the data is either N for no or Y for yes. Therefore, we do not know exactly who the other person that was stopped is. There are no recommended data splits. There are no errors, sources of noise, or redundancies in the dataset. The dataset is self-contained. No, the dataset does not contain data that might be considered confidential. The dataset may cause some anxiety if the person viewing has had a similar experience to being subject to the stop and frisk program. Yes, the dataset contains data on the people who were stopped.\n",
    "\n",
    "The dataset does identify subpopulations by age, sex, and race. There are significantly more males who were stopped in 2019 than females. The most frequently stopped race was Black, with White being the second most stopped race. Asian/Pacific Islander was the least stopped race, excluding the null race category. The most frequently stopped age group was 20-30 year olds with 30-40 year olds as the second most stopped age group. It is not possible to identify individuals based on our cleaned dataset. The dataset contains data on racial origins.\n",
    "\n",
    "**Collection Process**\n",
    "\n",
    "The data associated with each instance was acquired by the officer who initiated the stop. Police reports were used to collect data. The NYPD was involved in the data collection process, compensated by taxpayer dollars. The data was collected from the start to end of 2019. To our knowledge, no ethical review processes were conducted. Yes, the dataset relates to people who were stopped.\n",
    "\n",
    "**Pre-Processing / Cleaning / Labeling**\n",
    "\n",
    "There was no pre-processing done to the data prior to our data cleaning. Our data cleaning included dropping unecessary and empty columns, renaming / reorganizing columns, dropping instances with ages outside our decided age range, dropping instances with an empty boro column, dropping instances with an observation duration of 999, and converting all columns eligible to be represented by True/False values to those boolean values. Then, a 25% random sample was drawn from this population data. A much more detailed account of the data cleaning process is available in the Section X. Appendix: Data Cleaning Description.\n",
    "\n",
    "**Uses**\n",
    "\n",
    "To our knowledge, the dataset has not been used for any tasks already. The dataset could be used for further research into stop and frisk incidents in 2019 or could be compared to other years. There are no tasks for which the dataset should not be used. There is nothing about the composition of the dataset or the way it was collected and preprocessed that might impact future uses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset #3: Criminal Offense Data (2009-2019)\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "This dataset was created in order to aggregate the annual crime data in New York City, including felony, misdemeanor, and violation offenses from 2009 to 2019. \n",
    "\n",
    "This dataset was composed from four individual datasets created by the New York Police Department (NYPD) for the public. The four original datasets consisted of data regarding the Seven Major Felony Offenses, Non Major Felony Offenses, Misdemeanor Offenses, and Violation Offenses. \n",
    "\n",
    "The creation of the four individual datasets were funded by the government via taxpayer dollars allocated to the NYPD.\n",
    "\n",
    "**Composition**\n",
    "\n",
    "The instances that comprise the dataset represent the aggregate counts of each type of criminal offense within the categories of Major Felonies, Nonmajor Felonies, Misdemeanors, and Violations reported in NYC by the NYPD by year (2009-2019). There are 374 instances in total in the dataset, indexed by type of offense and year of occurrence. Under each year, there are entries consistent with 34 types of offenses, each of which are considered either a Major Felony, Nonmajor Felony, Misdemeanor, or Violation Offense under New York state and/or Federal Law. For each type of offense, there are 11 entries, one for each year from 2009 to 2019. The dataset is not a sample and contains all possible instances. However, the four datasets from which this one was composed were likely created by aggregating data on each individual criminal offense reported by the NYPD in the relevant time period. Each instance in the dataset consists of an aggregated count for each type of criminal offense reported by the NYPD by year (2009-2019). Each instance is labeled by type of criminal offense, category of offense (Major Felony, Nonmajor Felony, Misdemeanor, or Violation) and year. No information is missing from individual instances. Each instance of data includes the necessary information and context. However, each instance is merely the count per type of criminal offense per year, not the individual instances of crime themselves. The relationship between individual instances is made explicit. The count of each type of criminal offense can be compared across years (2000-2020). Likewise, the count of each type of criminal offense can be compared within each year.\n",
    "\n",
    "There are no recommended data splits. There are no errors, sources of noise, or redundancies in the dataset. This dataset is concise and clean. The dataset is not self-contained. The dataset was created by combining four other datasets, each of which linked to external datasets as sources of information.\n",
    "\n",
    "The dataset does not contain data that might be considered confidential. All the information in the dataset is public NYPD data. The dataset does not contain data that would likely be perceived as offensive, insulting, threatening, or anxiety-inducing if viewed directly. However, given that the data pertains to violent crimes, one could possibly have an emotional response to the data, though it is unlikely. The dataset does not relate to people.\n",
    "\n",
    "**Collection Process**\n",
    "\n",
    "The data associated with each instance is not directly observable, as it was acquired by aggregating data collected from external datasets. The four dataset used to compose this one were produced by the NYPD from their own reporting, so the data was likely validated internally. The data was collected by aggregating individual instances of criminal offenses collected by manual human curation via NYPD reports. The police officers who collected this data did so as part of the crime-reporting operations and public disclosure. These officials of the NYPD were likely compensated in their capacity as a state employee. The data was collected over the course of roughly a decade, 2009-2019.\n",
    "\n",
    "No ethical review processes were conducted as the data collected is the direct byproduct of NYPD operations. The dataset does not relate to people, as each instance of criminal activity is merely one point in an aggregate count and is removed from any individually identifying factors.\n",
    "\n",
    "**Pre-Processing / Cleaning / Labeling**\n",
    "\n",
    "The four datasets used to create the final dataset were likely pre-processed by the NYPD to produce such clean data. Even still, in the process of combining the four datasets, each one had to first be cleaned to eliminate data from years that do not pertain to the timeline of interest within the guiding research question. The four datasets also had to be cleaned to be properly formatted, which included renaming the columns, removing missing values, renaming the offense types, and casting all numbers as ints. Finally, a column was added to indicate the category of offense (Major Felony, Nonmajor Felony, Misdemeanor, or Violation) for reference in the final dataset.\n",
    "\n",
    "The raw data was saved both in the original file formats and as DataFrames (major_felony_raw, nonmajor_felony_raw, misdemeanor_raw, and violation_raw). The dataset was cleaned using Python within a Jupyter Lab Notebook. \n",
    "\n",
    "\n",
    "**Uses**\n",
    "\n",
    "This dataset has not been used for any tasks already. However, the four datasets from which it was composed may have been used for other purposes, though the website in which these datasets were found (ny.gov) does not provide any information regarding any prior use. The dataset could possibly be used for other analyses that pertain to the changes in felony crimes rates in NYC over the last decade. \n",
    "\n",
    "Unfortunately, the provider of the four datasets used to create this one (the NYPD) does not specifically detail their process of collecting and preprocessing the data, so we cannot infer how future users may be impacted by their decisions. In the process of cleaning the final dataset, certain offense types were renamed for ease and consistency. While minimal context was lost, future users would benefit from examining the four datasets from which the final was composed in order to see the offense types as listed by the NYPD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset #4: Aggregated Stop, Question, and Frisk Instances by Year (2009-2019)\n",
    "\n",
    "**Motivation**\n",
    "\n",
    "Enter text here...\n",
    "\n",
    "**Composition**\n",
    "\n",
    "Enter text here...\n",
    "\n",
    "**Collection Process**\n",
    "\n",
    "Enter text here...\n",
    "\n",
    "**Pre-Processing / Cleaning / Labeling**\n",
    "\n",
    "Enter text here...\n",
    "\n",
    "**Uses**\n",
    "\n",
    "Enter text here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Preregistration Statement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Annual NYC Crime Data from 2009 to 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to determine the efficacy of the NYPD's stop-and-frisk program, we must first determine whether there was a relationship between the number of stops and the number of reported crimes in New York during the period of interest (2009-2019). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 1: Linear Regression Analysis of Stop-and Frisk's Efficacy\n",
    "\n",
    "We will perform a linear regression analysis using the number of stops recorded each year as the predictor variable, X, and the number of reported crimes in NYC (in the relevant categories of weapons and contrand) as the criterion variable, Y.\n",
    "\n",
    "This analysis will allow us to establish a relationship between the stop-and-frisk program and crime rates in NYC. With this test we hope to distinguish a decrease in crime due stop-and-frisk from a decrease due to external factors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another analysis we will consider to determine the program's efficacy is a calculation of the percentage of civilians who were stopped that were actually charged with the crime suspected. This metric will provide an understanding of how purposeful and productive the program actually was."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing the Disproportionate Racial Targeting of Stop-and-Frisk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 2: Logistic Regression Analysis of Who is Frisked"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have five possible predictor variables for the logistic regression. Most of them are categorical predictors (race, sex, and borough), and only two are continuous numeric (age and observation duration). We plan to bin the ages and observation duration values and use get_dummies() to map them to indicator variables. The method we will use to bin ages is ages 13-17 inclusive (Teenagers), 18-25 (Young Adults), 26-39 (Adults), 40-65 (Middle-age), 66+ (Seniors). In order to determine the bins for the observation duration values, we must look at the distribution and identify the best partitions for the data.\n",
    "\n",
    "1. Bootstrapping.\n",
    "Need to identify distribution of obs_duration data to determine partitions to get_dummies()\n",
    "Bootstrap, get the 95% confidence interval\n",
    "Then, determine the bins for obs_duration\n",
    "\n",
    "Once all of the variables are in binary/categorical form, we will run a logistic regression using all five predictors and who has been frisked (entered as a True/False value in the table) as the target variable.\n",
    "\n",
    "2. Who is Frisked?\n",
    "Run a Logistic Regression to Predict who is Frisked\n",
    "Use all predictor variables\n",
    "Race - get_dummies() (7 race categories)\n",
    "Age - bin ages by 13-17 inclusive (Teenagers), 18-25 (Young Adults), 26-39 (Adults), 40-65 (Middle-age), 66+ (Seniors)\n",
    "Sex - female = 1, male = 0\n",
    "Borough - get_dummies() (5 borough categories)\n",
    "Obs_duration- bin times based on bootstrap\n",
    "Plot frisked = 1, not frisked = 0\n",
    "Get the prediction accuracy by applying the model to the test data\n",
    "Plot coefficients for each predictor, analyze those coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test 3: Run Chi-Squared Test to Improve Predictors for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we analyze the accuracy of our initial logistic regression model, we will investigate whether we can improve the model by looking at whether there is a significant dependency between a predictor variable and the target variable. We will see if there is any predictors that should be omitted from the model if they have an insignificant relationship with the target variable.\n",
    "\n",
    "Run Chi-Squared Tests to Improve the Predictors Chosen for the Logistic Regression of who is Frisked:\n",
    "\n",
    "Chi-Squared Tests to Run:\n",
    "Is there a dependence relation between race and who is frisked?\n",
    "Is there a dependence relation between age and who is frisked?\n",
    "Is there a dependence relation between sex and who is frisked?\n",
    "Is there a dependence relation between boro and who is frisked?\n",
    "Is there a dependence relation between observation duration (obs_duration) and who is frisked?\n",
    "\n",
    "The predictors with significant p-values will remain in the logistic regression model.\n",
    "\n",
    "If we do find that there are some predictors that should be omitted, then we will omit them and re-run the logistic regression and see how much the model has improved. Based on which predictors are significant, will inform us what traits/factors have the greatest impact on whether someone is frisked.\n",
    "\n",
    "Depending on the results of the annual NYC crime data and the most influential predictors for determining who is frisked, we will look at those results in conjunction with each other. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Data Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression Analysis of Stop-and Frisk's Efficacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Linear Regression Model to Predict the Number of Annual Offenses\n",
    "\n",
    "This linear regression model will predict the total number of criminal offenses reported annually in NYC from the number of stops recorded annually in NYC. \n",
    "\n",
    "The purpose of this model is to investigate whether the NYPD's Stop and Frisk Program had a tangible and quantifiable effect on criminal activity reported in NYC from 2011 to 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-a63c667d4e09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Load Crime Offenses dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnyc_crime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nyc_crime'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnyc_crime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnyc_crime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mnyc_crime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Load Crime Offenses dataset\n",
    "nyc_crime = pd.read_csv('nyc_crime')\n",
    "nyc_crime = nyc_crime.drop(columns = 'Unnamed: 0')\n",
    "nyc_crime.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of criminal offenses recorded each year (2011-2019)\n",
    "offense_types = nyc_crime['offense'].count()\n",
    "\n",
    "annual_offenses = []\n",
    "for year in nyc_crime.columns[3:12]:\n",
    "    count = 0\n",
    "    for offense in range(offense_types):\n",
    "        count = count + nyc_crime[year][offense]\n",
    "        \n",
    "    annual_offenses = annual_offenses + [count]\n",
    "print('Total Criminal Offenses Reported Annually (2011-2019): ')\n",
    "print(annual_offenses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the total number of stops reported - Dataset #4\n",
    "\n",
    "# Source: https://www.nyclu.org/en/stop-and-frisk-data\n",
    "\n",
    "annual_stops = np.array([685724, 532911, 191851, 45787, 22565, 12404, 11629, 11008, 13450])\n",
    "\n",
    "print('Total Stops Reported Annually (2011-2019): ')\n",
    "print(annual_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing the Linear Regression to Predict Total Offenses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear regression with number of stops each year as predictor var X and total number of offenses as criterion var y\n",
    "\n",
    "linear_model = LinearRegression().fit(annual_stops.reshape(-1,1), annual_offenses)\n",
    "\n",
    "# Scatterplot of observed values and predicted values\n",
    "plt.scatter(annual_stops, annual_offenses, label = 'Observed Values')\n",
    "predicted_offenses = linear_model.predict(annual_stops.reshape(-1,1))\n",
    "plt.scatter(annual_stops, predicted_offenses, label = 'Predicted Values')\n",
    "plt.title('Annual Stops vs. Annual Offenses')\n",
    "plt.xlabel('Annual Stops')\n",
    "plt.ylabel('Annual Offenses')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print slope, intercept, and coefficient of determination (r^2)\n",
    "print('Estimated slope: {:.2f}'.format(linear_model.coef_[0]))\n",
    "print('Estimated intercept: {:.2f}'.format(linear_model.intercept_))\n",
    "print('Coefficient of Determination (r^2): {:.2f}'.format(linear_model.score(annual_stops.reshape(-1,1), annual_offenses)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated slope of this linear regression model is 0.14. This value indicates that as the number of annual stops increases by 1, the number of annual offenses is predicted to increase by 0.14.\n",
    "\n",
    "We see here that number of stops is a moderate predictor of number of offenses overall. \n",
    "The r<sup>2</sup> value (the coefficient of determination) is 0.55, indicating that only 55% of the variation in the number of total annual offenses can be predicted by the number of annual stops."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's narrow this down to the two offenses specifically targeted by stop and frisk, drugs and weapons.\n",
    "\n",
    "The two illegal activities specifically targeted by the NYPD's Stop and Frisk Program are drug and weapon possession. Therefore, it is necessary to perform another linear regression analysis specifically modeling the relationship between the total number of stops reported annually and the number of drug & weapon offenses reported annually in NYC during this period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a Linear Regression Model to Predict the Number of Annual Drug & Weapons Offenses\n",
    "\n",
    "This linear regression model will predict the total number of criminal drug & weapons offenses reported annually in NYC from the number of stops recorded annually in NYC. The stop-and-frisk program primarily targets civilians suspected of drug- and/or weapons-related offenses, so one might anticipate that the number of stops to have a greater effect on the rates of these specific crimes than criminal offenses overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gathering the Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of drug & weapons offenses reported annually (2011-2019)\n",
    "drugs_weapons = nyc_crime.loc[(nyc_crime['offense'] == 'drugs') | (nyc_crime['offense'] == 'weapons')]\n",
    "\n",
    "annual_drugsWeapons = []\n",
    "for year in nyc_crime.columns[3:12]:\n",
    "    count = 0\n",
    "    for offense in drugs_weapons[year]:\n",
    "        count = count + offense\n",
    "    annual_drugsWeapons = annual_drugsWeapons + [count]\n",
    "        \n",
    "print('Drug & Weapons Offenses Reported Annual (2011-2019):')\n",
    "print(annual_drugsWeapons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing the Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform linear regression with number of stops each year as predictor var X and number of drug & weapons offenses as criterion var y\n",
    "dw_linear_model = LinearRegression().fit(annual_stops.reshape(-1,1), annual_drugsWeapons)\n",
    "\n",
    "# Scatterplot of observed values and predicted values\n",
    "plt.scatter(annual_stops, annual_drugsWeapons, label = 'Observed Values')\n",
    "predicted_dw = dw_linear_model.predict(annual_stops.reshape(-1,1))\n",
    "plt.scatter(annual_stops, predicted_dw, label = 'Predicted Values')\n",
    "plt.title('Annual Stops vs. Annual Offenses (Drug and Weapon Charges)')\n",
    "plt.xlabel('Annual Stops')\n",
    "plt.ylabel('Annual Offenses')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print slope, intercept, and r^2\n",
    "print('Estimated slope: {:.2f}'.format(dw_linear_model.coef_[0]))\n",
    "print('Estimated intercept: {:.2f}'.format(dw_linear_model.intercept_))\n",
    "print('r^2: {:.2f}'.format(dw_linear_model.score(annual_stops.reshape(-1,1), annual_drugsWeapons)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of stops serves as a stronger predictor of the number of drug & weapons offenses rather than total offenses.\n",
    "\n",
    "The r^2 value (the coefficient of determination) of this model is 0.70, indicating that 70% of the variation in the number of annual offenses can be predicted by the number of annual stops for drugs & weapons. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop, Question, and Frisk Logistic Regression Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Bootstrapping Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap(df, colname):\n",
    "    \"\"\" Calculate 5000 bootstraps on the colname column in df for a given year. \"\"\"\n",
    "    data= df[colname]\n",
    "    sample_size= data.size\n",
    "    \n",
    "    # Bootstrap 5000 times. For each resample, calculate the mean of the sample.\n",
    "    bootstraps= []\n",
    "\n",
    "    for i in np.arange(0, 5001):\n",
    "        resample= np.random.choice(data, sample_size)\n",
    "        resample_mean= np.average(resample)\n",
    "        bootstraps= np.append(bootstraps, resample_mean)\n",
    "        \n",
    "    return bootstraps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Contingency Table Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contingency_table(df, col1_name, col2_name):\n",
    "    \"\"\" Generate a contingency table using the DataFrame df, and the 2 column names provided (Strings) col1_name and col2_name. \"\"\"\n",
    "    \n",
    "    col_a_categories= df[col1_name].unique()\n",
    "    col_a= pd.Categorical(df[col1_name], categories= col_a_categories)\n",
    "    \n",
    "    col_b_categories= df[col2_name].unique()\n",
    "    col_b= pd.Categorical(df[col2_name], categories= col_b_categories)\n",
    "    \n",
    "    return pd.crosstab(col_a, col_b, rownames= [col1_name], colnames= [col2_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) Calculate Chi-Squared P-Value Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_chi_squared(df, col1_name, col2_name):\n",
    "    \"\"\" Calculate the chi-squared p-value of 2 columns from a given DataFrame. \"\"\"\n",
    "    cTable= get_contingency_table(df, col1_name, col2_name)\n",
    "    \n",
    "    return stats.chi2_contingency(cTable)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Logistic Regression Prediction Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LogR(logdf, getbinary, targetvar):\n",
    "    \"\"\" creates a df with only vars of interest with indicator variables, runs a logistic regression on that and returns a prediction accuracy percentage.\n",
    "    Parameters: logdf is a df with only the variables in interest for logistic regression. \n",
    "    targetvar is a string of the target variable for the logistic regression.\n",
    "    getbinary is a dataframe with columns that need to be given indicator variables and will then be added on to the logistic_data df.\n",
    "      \"\"\"\n",
    "    newgetbinary = pd.get_dummies(getbinary)               # creating new df for log regression\n",
    "    newlogdf = pd.concat([logdf, newgetbinary], axis=1)\n",
    "\n",
    "    predictors = list(newlogdf.columns)\n",
    "    predictors.remove(targetvar)\n",
    "    train,test = train_test_split(newlogdf, test_size=0.2, random_state=15)\n",
    "    logmod= LogisticRegression(solver='lbfgs', max_iter=1000);\n",
    "    logmod.fit(train[predictors], train[targetvar])  \n",
    "    tepreds=logmod.predict(test[predictors])\n",
    "    \n",
    "    return \"the prediction accuracy as a percentage is {:.1f}.\".format(np.sum(tepreds==test[targetvar])/len(tepreds)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Predictors Coefficients Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqf2011= pd.read_csv('sqf2011_final_sample')\n",
    "sqf2019= pd.read_csv('sqf2019_final_sample')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def coeffs(logdf, getbinary, targetvar): \n",
    "    \"\"\"Prints a list of coefficients and displays a plot of the coefficents in descending order.\n",
    "    Parameters: logdf is a dataframe with only variables of intereset for logistic regression.\n",
    "    getbinary is a dataframe with variables that need to be given indicator variables for running a  logistic regression.\n",
    "    targetvar is a string of the target variable name for the logistic regression.\n",
    "    \"\"\"\n",
    "    newgetbinary = pd.get_dummies(getbinary)\n",
    "    newlogdf = pd.concat([logdf, newgetbinary], axis=1)  \n",
    "    predictors = list(newlogdf.columns);\n",
    "    predictors.remove(targetvar);\n",
    "    train,test = train_test_split(newlogdf, test_size=0.2, random_state=15);\n",
    "    logmod = LogisticRegression(solver='lbfgs', max_iter=1000);\n",
    "    logmod.fit(train[predictors], train[targetvar]);\n",
    "    \n",
    "    # print the coefficients legibly\n",
    "    for i, predictor in enumerate(predictors):\n",
    "        print(f'{logmod.coef_[0,i]:.3f}\\t{predictor}')\n",
    "\n",
    "    # plot predictors by lean\n",
    "    df = pd.DataFrame({'coef':logmod.coef_[0]}, index=predictors)\n",
    "    df.sort_values(by='coef').plot.barh(legend=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frisked 2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Unoptimized **Frisked 2011** Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqf2011copy1 = sqf2011.copy()\n",
    "sqf2019copy1 = sqf2019.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flogdf2011 = sqf2011copy1[['frisked', 'obs_duration', 'age']]  # first, just the 'frisked', 'obs_duration', 'age' columns\n",
    "fgetbinary2011 = sqf2011copy1[['sex', 'race', 'boro']]\n",
    "print(\"Using the model to predict whether one is frisked in the 2011 test set:\")\n",
    "LogR(flogdf2011, fgetbinary2011, 'frisked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Which **Frisked 2011 Predictors** are Significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Observation Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_duration_frisked= sqf2011.loc[sqf2011['frisked'] == True]\n",
    "\n",
    "obs_bootstraps2011= bootstrap(obs_duration_frisked, 'obs_duration')\n",
    "\n",
    "plt.figure(figsize= (8, 6))\n",
    "plt.hist(obs_bootstraps2011)\n",
    "plt.xlabel('Mean Observation Duration (Minutes)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mean Observation Duration (Minutes) Distribution (2011)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval\n",
    "left_bound = np.percentile(obs_bootstraps2011, 2.5)\n",
    "right_bound = np.percentile(obs_bootstraps2011, 97.5)\n",
    "\n",
    "conf_interval_print= \"The middle 95% confidence interval from the bootstrap estimates is ({:.2f},  {:.2f}) minutes.\".format(left_bound, right_bound)\n",
    "print(conf_interval_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fObs_test_stat2011= np.average(sqf2011.obs_duration)\n",
    "fObs_pVal2011= sum(obs_bootstraps2011 >= fObs_test_stat2011) / len(obs_bootstraps2011)\n",
    "fObs_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_frisked= sqf2011.loc[sqf2011['frisked'] == True]\n",
    "\n",
    "age_bootstraps2011= bootstrap(age_frisked, 'age')\n",
    "\n",
    "plt.figure(figsize= (8, 6))\n",
    "plt.hist(age_bootstraps2011)\n",
    "plt.xlabel('Mean Age (Years)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mean Age (Years) Distribution (2011)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval\n",
    "left_bound = np.percentile(age_bootstraps2011, 2.5)\n",
    "right_bound = np.percentile(age_bootstraps2011, 97.5)\n",
    "\n",
    "conf_interval_print= \"The middle 95% confidence interval from the bootstrap estimates is ({:.2f},  {:.2f}) minutes.\".format(left_bound, right_bound)\n",
    "print(conf_interval_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fAge_test_stat2011= np.average(sqf2011.age)\n",
    "fAge_pVal2011= sum(age_bootstraps2011 >= fAge_test_stat2011) / len(age_bootstraps2011)\n",
    "fAge_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fSex_pVal2011= calc_chi_squared(sqf2011, 'frisked', 'sex')\n",
    "fSex_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fRace_pVal2011= calc_chi_squared(sqf2011, 'frisked', 'race')\n",
    "fRace_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (5) Boro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fBoro_pVal2011= calc_chi_squared(sqf2011, 'frisked', 'boro')\n",
    "fBoro_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on the Chi square tests all the predictor vairbales have a statistically significant association with whether someone is Frisked in 2011."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs(flogdf2011, fgetbinary2011, 'frisked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot, the four most influential variables on getting frisked in 2011 are being male, in the Bronx or Queens, and Black."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frisked 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Unoptimized **Frisked 2019** Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flogdf2019 = sqf2019copy1[['frisked', 'obs_duration', 'age']]  # first, just the 'frisked', 'obs_duration', 'age' columns\n",
    "fgetbinary2019 = sqf2019copy1[['sex', 'race', 'boro']]\n",
    "print(\"Using the model to predict whether one is frisked in the 2019 test set:\")\n",
    "LogR(flogdf2019, fgetbinary2019, 'frisked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs(flogdf2019, fgetbinary2019, 'frisked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot of coefficients from the inital logistic regression, the most influential factors on whether someone is frisked in 2019 are if they are a male, in the Bronx or Brooklyn, and Black."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Which **Frisked 2019 Predictors** are Significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Observation Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_duration_frisked= sqf2019.loc[sqf2019['frisked'] == True]\n",
    "\n",
    "obs_bootstraps2019= bootstrap(obs_duration_frisked, 'obs_duration')\n",
    "\n",
    "plt.figure(figsize= (8, 6))\n",
    "plt.hist(obs_bootstraps2019)\n",
    "plt.xlabel('Mean Observation Duration (Minutes)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mean Observation Duration (Minutes) Distribution (2019)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval\n",
    "left_bound = np.percentile(obs_bootstraps2019, 2.5)\n",
    "right_bound = np.percentile(obs_bootstraps2019, 97.5)\n",
    "\n",
    "conf_interval_print= \"The middle 95% confidence interval from the bootstrap estimates is ({:.2f},  {:.2f}).\".format(left_bound, right_bound)\n",
    "print(conf_interval_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fObs_test_stat2019= np.average(sqf2019.obs_duration)\n",
    "fObs_pVal2019= sum(obs_bootstraps2019 >= fObs_test_stat2019) / len(obs_bootstraps2019)\n",
    "fObs_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_frisked= sqf2019.loc[sqf2019['frisked'] == True]\n",
    "\n",
    "age_bootstraps2019= bootstrap(age_frisked, 'age')\n",
    "\n",
    "plt.figure(figsize= (8, 6))\n",
    "plt.hist(age_bootstraps2019)\n",
    "plt.xlabel('Mean Age (Years)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mean Age (Years) Distribution (2019)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval\n",
    "left_bound = np.percentile(age_bootstraps2019, 2.5)\n",
    "right_bound = np.percentile(age_bootstraps2019, 97.5)\n",
    "\n",
    "conf_interval_print= \"The middle 95% confidence interval from the bootstrap estimates is ({:.2f},  {:.2f}) minutes.\".format(left_bound, right_bound)\n",
    "print(conf_interval_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fAge_test_stat2019= np.average(sqf2019.obs_duration)\n",
    "fObs_pVal2019= sum(obs_bootstraps2019 >= fAge_test_stat2019) / len(obs_bootstraps2019)\n",
    "fObs_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fSex_pVal2019= calc_chi_squared(sqf2019, 'frisked', 'sex')\n",
    "fSex_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fRace_pVal2019= calc_chi_squared(sqf2019, 'frisked', 'race')\n",
    "fRace_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (5) Boro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fBoro_pVal2019= calc_chi_squared(sqf2019, 'frisked', 'boro')\n",
    "fBoro_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on our chi squared tests, sex, race and boro have a statistically significant association with whether someone is frisked in 2019. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) **Optimized Frisked 2019** Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optflogdf2019 = sqf2019copy1[['frisked']]\n",
    "optfgetbinary2019 = sqf2019copy1[['sex', 'race', 'boro']]\n",
    "print(\"Using the *optimized* model to predict whether one is frisked in the 2019 test set:\")\n",
    "LogR(optflogdf2019, optfgetbinary2019, 'frisked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs(optflogdf2019, optfgetbinary2019, 'frisked')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot of coefficients from the optimized logistic regression, the four variables with the most impact on whether one is frisked in 2019 are being a male, in the Bronx or Brooklyn, and Black."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial prediciton accuracy of our logistic regression is 63.8 percent while our optimized prediciton accuracy is 62.2 percent. Our chi-squared test determined that observation duration and age have no significant association with whether one is frisked in 2019. The bar plot also shows that obs_duration and age have coefficients very close to 0, meaning they have little to no effeect on whether one is friksed in 2019. Our optimized logistic regression model is run using two less predictor variables and therefore has a lower prediciton accuracy than our inital model. However, the prediciton accuracies only differ by 1.6 percent which implies our optimized model is about as accurate as our inital model, even with less predictor variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searched 2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Unoptimized **Searched 2011** Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slogdf2011 = sqf2011copy1[['searched', 'obs_duration', 'age']]  # first, just the 'searched', 'obs_duration', 'age' columns\n",
    "sgetbinary2011 = sqf2011copy1[['sex', 'race', 'boro']]\n",
    "LogR(slogdf2011, sgetbinary2011, 'searched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs(slogdf2011, sgetbinary2011, 'searched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Which **Searched 2011 Predictors** are Significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Observation Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_duration_searched= sqf2011.loc[sqf2011['searched'] == True]\n",
    "\n",
    "obs_bootstraps2011= bootstrap(obs_duration_searched, 'obs_duration')\n",
    "\n",
    "plt.figure(figsize= (8, 6))\n",
    "plt.hist(obs_bootstraps2011)\n",
    "plt.xlabel('Mean Observation Duration (Minutes)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mean Observation Duration (Minutes) Distribution (2011)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval\n",
    "left_bound = np.percentile(obs_bootstraps2011, 2.5)\n",
    "right_bound = np.percentile(obs_bootstraps2011, 97.5)\n",
    "\n",
    "conf_interval_print= \"The middle 95% confidence interval from the bootstrap estimates is ({:.2f},  {:.2f}) minutes.\".format(left_bound, right_bound)\n",
    "print(conf_interval_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sObs_test_stat2011= np.average(sqf2011.obs_duration)\n",
    "sObs_pVal2011= sum(obs_bootstraps2011 >= sObs_test_stat2011) / len(obs_bootstraps2011)\n",
    "sObs_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_searched= sqf2011.loc[sqf2011['searched'] == True]\n",
    "\n",
    "age_bootstraps2011= bootstrap(age_searched, 'age')\n",
    "\n",
    "plt.figure(figsize= (8, 6))\n",
    "plt.hist(age_bootstraps2011)\n",
    "plt.xlabel('Mean Age (Years)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mean Age (Years) Distribution (2011)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval\n",
    "left_bound = np.percentile(age_bootstraps2011, 2.5)\n",
    "right_bound = np.percentile(age_bootstraps2011, 97.5)\n",
    "\n",
    "conf_interval_print= \"The middle 95% confidence interval from the bootstrap estimates is ({:.2f},  {:.2f}) minutes.\".format(left_bound, right_bound)\n",
    "print(conf_interval_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sAge_test_stat2011= np.average(sqf2011.age)\n",
    "sAge_pVal2011= sum(age_bootstraps2011 >= sAge_test_stat2011) / len(age_bootstraps2011)\n",
    "sAge_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sSex_pVal2011= calc_chi_squared(sqf2011, 'searched', 'sex')\n",
    "sSex_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sRace_pVal2011= calc_chi_squared(sqf2011, 'searched', 'race')\n",
    "sRace_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (5) Boro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sBoro_pVal2011= calc_chi_squared(sqf2011, 'searched', 'boro')\n",
    "sBoro_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on our chi-squared tests, sex, race, and boro have a statistically significant association with whether one is searched in 2011."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) **Optimized Searched 2011** Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soptlogdf2011 = sqf2011copy1[['searched']]  # first, just the 'searched'\n",
    "soptgetbinary2011 = sqf2011copy1[['sex', 'race', 'boro']]\n",
    "print(\"Using the *optimized* model to predict whether one is searched in the 2011 test set:\")\n",
    "LogR(soptlogdf2011, soptgetbinary2011, 'searched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs(soptlogdf2011, soptgetbinary2011, 'searched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot, the four variables with the most impact on whether one is searched in 2011 are being being in Queens, White, male, and in Manhattan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial and optimized logistic regression prediciton accuracies are both 91.6 percent. Our chi-squared test determined that observation duration and age have no significant association with whether one is searched in 2011. The bar plot also shows that obs_duration and age have coefficients of 0.002, meaning they have extremely little to no effect on whether one is searched in 2011. Our optimized logistic regression model is run using two less predictor variables but the rounded prediciton accuracies are the same which indicates that observation duration anad age have almost no effect on whether one is searched in 2011. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searched 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Unoptimized **Searched 2019** Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slogdf2019 = sqf2019copy1[['searched', 'obs_duration', 'age']]  # first, just the 'searched', 'obs_duration', 'age' columns\n",
    "sgetbinary2019 = sqf2019copy1[['sex', 'race', 'boro']]\n",
    "LogR(slogdf2019, sgetbinary2019, 'searched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs(slogdf2019, sgetbinary2019, 'searched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Which **Searched 2019 Predictors** are Significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Observation Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_duration_searched= sqf2019.loc[sqf2019['searched'] == True]\n",
    "\n",
    "obs_bootstraps2019= bootstrap(obs_duration_searched, 'obs_duration')\n",
    "\n",
    "plt.figure(figsize= (8, 6))\n",
    "plt.hist(obs_bootstraps2019)\n",
    "plt.xlabel('Mean Observation Duration (Minutes)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mean Observation Duration (Minutes) Distribution (2019)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval\n",
    "left_bound = np.percentile(obs_bootstraps2019, 2.5)\n",
    "right_bound = np.percentile(obs_bootstraps2019, 97.5)\n",
    "\n",
    "conf_interval_print= \"The middle 95% confidence interval from the bootstrap estimates is ({:.2f},  {:.2f}).\".format(left_bound, right_bound)\n",
    "print(conf_interval_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sObs_test_stat2019= np.average(sqf2019.obs_duration)\n",
    "sObs_pVal2019= sum(obs_bootstraps2019 >= sObs_test_stat2019) / len(obs_bootstraps2019)\n",
    "sObs_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_searched= sqf2019.loc[sqf2019['searched'] == True]\n",
    "\n",
    "age_bootstraps2019= bootstrap(age_searched, 'age')\n",
    "\n",
    "plt.figure(figsize= (8, 6))\n",
    "plt.hist(age_bootstraps2019)\n",
    "plt.xlabel('Mean Age (Years)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mean Age (Years) Distribution (2019)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval\n",
    "left_bound = np.percentile(age_bootstraps2019, 2.5)\n",
    "right_bound = np.percentile(age_bootstraps2019, 97.5)\n",
    "\n",
    "conf_interval_print= \"The middle 95% confidence interval from the bootstrap estimates is ({:.2f},  {:.2f}) minutes.\".format(left_bound, right_bound)\n",
    "print(conf_interval_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sAge_test_stat2019= np.average(sqf2019.obs_duration)\n",
    "sObs_pVal2019= sum(obs_bootstraps2019 >= sAge_test_stat2019) / len(obs_bootstraps2019)\n",
    "sObs_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sSex_pVal2019= calc_chi_squared(sqf2019, 'searched', 'sex')\n",
    "sSex_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sRace_pVal2019= calc_chi_squared(sqf2019, 'searched', 'race')\n",
    "sRace_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (5) Boro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sBoro_pVal2019= calc_chi_squared(sqf2019, 'searched', 'boro')\n",
    "sBoro_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on chi squared tests, sex has a statistically significant association with whether someone is searched through the stop and frisk program in 2019. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) **Optimized Searched 2019** Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optslogdf2019 = sqf2019copy1[['searched']]  # first, just the 'searched'\n",
    "optsgetbinary2019 = sqf2019copy1[['sex']]\n",
    "print(\"Using the *optimized* model to predict whether one is searched in the 2019 test set:\")\n",
    "LogR(optslogdf2019, optsgetbinary2019, 'searched')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs(optslogdf2019, optsgetbinary2019, 'searched')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot, being a male is what has the most impact on whether one is searched in 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial logistic regression prediciton accuracy is 58.9 percent and our optimized prediciton accuracy is 59.3 percent. Our chi-squared test determined that observation duration, age, race, and borough have no significant association with whether one is searched in 2019. The bar plot also shows that obs_duration and age have coefficients very close to 0, meaning they have little to no effect on whether one is searched in 2019. Given that our optimized logistic regression model is run using only 1 predictor variable and that our optimized model has a higher prediction accuracy than our initial model, it can be inferred that the four un-used predictor variables in our model are not significant factors that influence whether one is searched"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrest Made 2011"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Unoptimized Logistic Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlogdf2011 = sqf2011copy1[['arstmade', 'obs_duration', 'age']]  # first, just the 'arstmade', 'obs_duration', 'age' columns\n",
    "amgetbinary2011 = sqf2011copy1[['sex', 'race', 'boro']]\n",
    "LogR(amlogdf2011, amgetbinary2011, 'arstmade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs(amlogdf2011, amgetbinary2011, 'arstmade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Which **Arrest Made 2011 Predictors** are Significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Observation Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_duration_arrest= sqf2011.loc[sqf2011['arstmade'] == True]\n",
    "\n",
    "obs_bootstraps2011= bootstrap(obs_duration_arrest, 'obs_duration')\n",
    "\n",
    "plt.figure(figsize= (8, 6))\n",
    "plt.hist(obs_bootstraps2011)\n",
    "plt.xlabel('Mean Observation Duration (Minutes)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mean Observation Duration (Minutes) Distribution (2011)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval\n",
    "left_bound = np.percentile(obs_bootstraps2011, 2.5)\n",
    "right_bound = np.percentile(obs_bootstraps2011, 97.5)\n",
    "\n",
    "conf_interval_print= \"The middle 95% confidence interval from the bootstrap estimates is ({:.2f},  {:.2f}) minutes.\".format(left_bound, right_bound)\n",
    "print(conf_interval_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aObs_test_stat2011= np.average(sqf2011.obs_duration)\n",
    "aObs_pVal2011= sum(obs_bootstraps2011 >= aObs_test_stat2011) / len(obs_bootstraps2011)\n",
    "aObs_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_arrest= sqf2011.loc[sqf2011['arstmade'] == True]\n",
    "\n",
    "age_bootstraps2011= bootstrap(age_arrest, 'age')\n",
    "\n",
    "plt.figure(figsize= (8, 6))\n",
    "plt.hist(age_bootstraps2011)\n",
    "plt.xlabel('Mean Age (Years)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mean Age (Years) Distribution (2011)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval\n",
    "left_bound = np.percentile(age_bootstraps2011, 2.5)\n",
    "right_bound = np.percentile(age_bootstraps2011, 97.5)\n",
    "\n",
    "conf_interval_print= \"The middle 95% confidence interval from the bootstrap estimates is ({:.2f},  {:.2f}) minutes.\".format(left_bound, right_bound)\n",
    "print(conf_interval_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aAge_test_stat2011= np.average(sqf2011.age)\n",
    "aAge_pVal2011= sum(age_bootstraps2011 >= aAge_test_stat2011) / len(age_bootstraps2011)\n",
    "aAge_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aSex_pVal2011= calc_chi_squared(sqf2011, 'arstmade', 'sex')\n",
    "aSex_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aRace_pVal2011= calc_chi_squared(sqf2011, 'arstmade', 'race')\n",
    "aRace_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (5) Boro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aBoro_pVal2011= calc_chi_squared(sqf2011, 'arstmade', 'boro')\n",
    "aBoro_pVal2011 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on the chi-squared test, sex, race, and boro have a statisitcally significant association with whether one is arrested throug stop and frisk program in 2011."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (c) **Optimized Arrest Made 2011** Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optamlogdf2011 = sqf2011copy1[['arstmade']]  # first, just the 'arstmade'\n",
    "optamgetbinary2011 = sqf2011copy1[['sex', 'race', 'boro']]\n",
    "print(\"Using the *optimized* model to predict whether one is arrested in the 2011 test set:\")\n",
    "LogR(optamlogdf2011, optamgetbinary2011, 'arstmade')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs(optamlogdf2011, optamgetbinary2011, 'arstmade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot, the four variables that have the most impact on whether one is arrested through a stop and frisk in 2011 are being a female, in Manhattan, American Indian/Alaskan Native, or White."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial and optimized logistic regression prediciton accuracies are both 93.7 percent. Our chi-squared test determined that observation duration and age have no significant association with whether an arrest is made on someone in 2011. The bar plot also shows that obs_duration and age have coefficients of 0.009 and 0.008, respectively, meaning they have extremely little to no effect on whether one is arrested in 2011. Our optimized logistic regression model is run using two less predictor variables but the rounded prediciton accuracies are the same which indicates that observation duration anad age have almost no effect on whether one is arrested in 2011."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arrest Made 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (a) Unoptimized **Arrest Made 2019** Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amlogdf2019 = sqf2019copy1[['arstmade', 'obs_duration', 'age']]  # first, just the 'arstmade', 'obs_duration', 'age' columns\n",
    "amgetbinary2019 = sqf2019copy1[['sex', 'race', 'boro']]\n",
    "LogR(amlogdf2019, amgetbinary2019, 'arstmade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the chi-squared tests, none of the predictor variables have a statistically significant association with whether one is arrested thorugh the stop and frisk program in 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs(amlogdf2019, amgetbinary2019, 'arstmade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot, the variables with the most influence on whether one is arrested through stop and frisk in 2019 are being American Indian/Alaskan Native, being in Staten Island, and being black. The factors that are least likely to get arrested through a stop and frisk in 2019 are if they are male, Asian/Pacific Islander, or in Brooklyn. As with the earlier models, observation duration and age have coefficients close to zero which mean they have no close to no effect on whether one is arrested through stop and frisk in 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (b) Which **Arrest Made 2019 Predictors** are Significant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (1) Observation Duration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_duration_arrest= sqf2019.loc[sqf2019['arstmade'] == True]\n",
    "\n",
    "obs_bootstraps2019= bootstrap(obs_duration_arrest, 'obs_duration')\n",
    "\n",
    "plt.figure(figsize= (8, 6))\n",
    "plt.hist(obs_bootstraps2019)\n",
    "plt.xlabel('Mean Observation Duration (Minutes)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mean Observation Duration (Minutes) Distribution (2019)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval\n",
    "left_bound = np.percentile(obs_bootstraps2019, 2.5)\n",
    "right_bound = np.percentile(obs_bootstraps2019, 97.5)\n",
    "\n",
    "conf_interval_print= \"The middle 95% confidence interval from the bootstrap estimates is ({:.2f},  {:.2f}).\".format(left_bound, right_bound)\n",
    "print(conf_interval_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aObs_test_stat2019= np.average(sqf2019.obs_duration)\n",
    "aObs_pVal2019= sum(obs_bootstraps2019 >= aObs_test_stat2019) / len(obs_bootstraps2019)\n",
    "aObs_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (2) Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_arrest= sqf2019.loc[sqf2019['arstmade'] == True]\n",
    "\n",
    "age_bootstraps2019= bootstrap(age_searched, 'age')\n",
    "\n",
    "plt.figure(figsize= (8, 6))\n",
    "plt.hist(age_bootstraps2019)\n",
    "plt.xlabel('Mean Age (Years)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Mean Age (Years) Distribution (2019)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 95% confidence interval\n",
    "left_bound = np.percentile(age_bootstraps2019, 2.5)\n",
    "right_bound = np.percentile(age_bootstraps2019, 97.5)\n",
    "\n",
    "conf_interval_print= \"The middle 95% confidence interval from the bootstrap estimates is ({:.2f},  {:.2f}) minutes.\".format(left_bound, right_bound)\n",
    "print(conf_interval_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aAge_test_stat2019= np.average(sqf2019.obs_duration)\n",
    "aObs_pVal2019= sum(obs_bootstraps2019 >= aAge_test_stat2019) / len(obs_bootstraps2019)\n",
    "aObs_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (3) Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aSex_pVal2019= calc_chi_squared(sqf2019, 'arstmade', 'sex')\n",
    "aSex_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (4) Race"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aRace_pVal2019= calc_chi_squared(sqf2019, 'arstmade', 'race')\n",
    "aRace_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (5) Boro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aBoro_pVal2019= calc_chi_squared(sqf2019, 'arstmade', 'boro')\n",
    "aBoro_pVal2019 < 0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Based on the chi-squared tests, none of the predictor variables have a statistically significant association with whether one is arrested thorugh the stop and frisk program in 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs(amlogdf2019, amgetbinary2019, 'arstmade')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the plot, the variables with the most influence on whether one is arrested through stop and frisk in 2019 are being American Indian/Alaskan Native, being in Staten Island, and being black. The factors that are least likely to get arrested through a stop and frisk in 2019 are if they are male, Asian/Pacific Islander, or in Brooklyn. As with the earlier models, observation duration and age have coefficients close to zero which mean they have no close to no effect on whether one is arrested through stop and frisk in 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Evaluation of Significance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Crime Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop, Question, and Frisk (2011) Logistic Regression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop, Question, and Frisk (2019) Logistic Regression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VI. Interpretations and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overall Crime Trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop, Question, and Frisk (2011) Logistic Regression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stop, Question, and Frisk (2019) Logistic Regression Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VII. Limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two groups of data we have are datasets of the NYPD's Stop and Frisk instances for 2011 and 2019, and general crime data including major and minor felonies, misdemeanors, and violations in NYC for the years 2009-2019. Our stop and frisk data is only for NYC so we cannot use it as a representation of stop and frisks in other locations. Since the Stop and Frisk data is collected by the police officers who make the stops, the data is dependent on police officers’ judgement and possible bias. Officers make stops when they observe others and have suspicion to frisk them or probable cause to search them. The data is therefore limited in part to the individual officer’s judgment. For example, one officer may find a pedestrian suspicious and frisk them while another officer may not make that same judgement. In this way, there may be gaps or inconsistencies in our data in what one considers to be suspicious activity because it varies among officers and the accepted conditions of suspicious activities may also change throughout the years. The stop data is also limited by whether or not the officer decides to report it. There may be more stops made than we actually have in the data and we will not be able to analyze those unrecorded stops. Another limitation of this data is the missing data in many of the instances of the datasets. During our data cleaning section, we had to drop many instances where the age, observation duration, or boro information was poorly recorded or empty entirely. Those dropped instances do not have a chance to be considered in our analysis. While we cannot come up with a definitive answer to whether or not those dropped instances will make a significant difference to our analysis, we still must recognize their lost potential to affect our analysis.\n",
    "\n",
    "The case for general crime data is slightly different when it comes to underreporting. Underreporting in a stop and frisk case is dependent on the officer while for general crime statistics, regular civilians may be involved in crime and choose not to report it. Reasons for the public underreporting crime can be due to a distrust in the police, whether one is an undocumented immigrant, or a prior settlement. According to an article written by the West Virginia University,  up to or above 90% or rape cases go unreported. The most common reason for not reporting rape are fear of not being believed, insecurity, and fear of getting into trouble.(1) We can conclude our data does not include all crime occurrences since it is dependent on civilians reporting or police officers reporting crime.\n",
    "\n",
    "(1)\"Rape Myths and Facts | wellwvu | West Virginia University\". Well.wvu.edu. 2013-09-24. Archived from the original on 2017-04-20."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VIII. Source Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Our GitHub Repository](https://github.com/leajv123/Data-Science-Project-2950-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IX. Acknowledgements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- INFO 2950: Introduction to Data Science Lecture Code.\n",
    "- INFO 2950: Introduction to Data Science Teaching Staff for answering questions during Class, Office Hours, and via email.\n",
    "- ORIE 1380: Data Science for All Homework and Lab Code (Spring 2020).\n",
    "- [Pandas](https://pandas.pydata.org/pandas-docs/stable/index.html), [NumPy](https://numpy.org/doc/), [SciKit-Learn](https://scikit-learn.org/stable/), and [MatPlotLib](https://matplotlib.org/) Documentation.\n",
    "- Consulted various [Stack Overflow](https://stackoverflow.com/) posts for clarification on how to use functions in the packages listed above.\n",
    "- Chi-Squared Test of Independence [Null and Alternative Hypothesis](https://www.statisticssolutions.com/non-parametric-analysis-chi-square/#:~:text=Alternative%20hypothesis%3A%20Assumes%20that%20there,association%20between%20the%20two%20variables.&text=If%20the%20observed%20chi%2Dsquare,interpret%20your%20analysis%20in%20minutes.) resources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X. Appendix: Data Cleaning Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset #1: Stop, Question, and Frisk (2011)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to be using 2 Stop and Frisk datasets for this project, one from 2011 and the other from 2019. Over time, the NYPD has modified the methods in which the data is recorded, causing the 2011 and 2019 datasets to have a lot of the same data stored in different ways. A large portion of the data cleaning will be adjusting column names and re-encoding data in order to make analysis easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dropped a list of column names that were irrelevant to our analysis, or columns that contained little to no data. The columns in this list are\n",
    "    - pct: PRECINCT OF STOP (FROM 1 TO 123)\n",
    "    - ser_num: UF250 SERIAL NUMBER\n",
    "    - recstat: RECORD STATUS\n",
    "    - inout: WAS STOP INSIDE OR OUTSIDE?\n",
    "    - trhsloc: WAS LOCATION HOUSING OR TRANSIT AUTHORITY\n",
    "    - typeofid: STOPPED PERSON'S IDENTIFICATION TYPE\n",
    "    - sumoffen: OFFENSE SUSPECT WAS SUMMONSED FOR\n",
    "    - compyear: COMPLAINT YEAR (IF COMPLAINT REPORT PREPARED)\n",
    "    - comppct: COMPLAINT PRECINCT (IF COMPLAINT REPORT PREPARED)\n",
    "    - offunif: WAS OFFICER IN UNIFORM?\n",
    "    - officrid: ID CARD PROVIDED BY OFFICER (IF NOT IN UNIFORM)\n",
    "    - adtlrept: WERE ADDITIONAL REPORTS PREPARED?\n",
    "    - radio: RADIO RUN\n",
    "    - ac_rept: ADDITIONAL CIRCUMSTANCES - REPORT BY VICTIM/WITNESS/OFFICER\n",
    "    - ac_inves: ADDITIONAL CIRCUMSTANCES - ONGOING INVESTIGATION\n",
    "    - rf_vcrim: REASON FOR FRISK - VIOLENT CRIME SUSPECTED\n",
    "    - rf_othsw: REASON FOR FRISK - OTHER SUSPICION OF WEAPONS\n",
    "    - ac_proxm: ADDITIONAL CIRCUMSTANCES - PROXIMITY TO SCENE OF OFFENSE\n",
    "    - rf_attir: REASON FOR FRISK - INAPPROPRIATE ATTIRE FOR SEASON\n",
    "    - cs_objcs: REASON FOR STOP - CARRYING SUSPICIOUS OBJECT\n",
    "    - cs_descr: REASON FOR STOP - FITS A RELEVANT DESCRIPTION\n",
    "    - cs_casng: REASON FOR STOP - CASING A VICTIM OR LOCATION\n",
    "    - cs_lkout: REASON FOR STOP - SUSPECT ACTING AS A LOOKOUT\n",
    "    - rf_vcact: REASON FOR FRISK-  ACTIONS OF ENGAGING IN A VIOLENT CRIME\n",
    "    - cs_cloth: REASON FOR STOP - WEARING CLOTHES COMMONLY USED IN A CRIME\n",
    "    - cs_drgtr: REASON FOR STOP - ACTIONS INDICATIVE OF A DRUG TRANSACTION\n",
    "    - ac_evasv: ADDITIONAL CIRCUMSTANCES - EVASIVE RESPONSE TO QUESTIONING\n",
    "    - ac_assoc: ADDITIONAL CIRCUMSTANCES - ASSOCIATING WITH KNOWN CRIMINALS\n",
    "    - cs_furtv: REASON FOR STOP - FURTIVE MOVEMENTS\n",
    "    - rf_rfcmp: REASON FOR FRISK - REFUSE TO COMPLY W OFFICER'S DIRECTIONS\n",
    "    - ac_cgdir: ADDITIONAL CIRCUMSTANCES - CHANGE DIRECTION AT SIGHT OF OFFICER\n",
    "    - rf_verbl: REASON FOR FRISK - VERBAL THREATS BY SUSPECT\n",
    "    - cs_vcrim: REASON FOR STOP - ACTIONS OF ENGAGING IN A VIOLENT CRIME\n",
    "    - cs_bulge: REASON FOR STOP - SUSPICIOUS BULGE\n",
    "    - cs_bulge: REASON FOR STOP - OTHER\n",
    "    - ac_incid: ADDITIONAL CIRCUMSTANCES - AREA HAS HIGH CRIME INCIDENCE\n",
    "    - ac_time: ADDITIONAL CIRCUMSTANCES - TIME OF DAY FITS CRIME INCIDENCE\n",
    "    - rf_knowl: REASON FOR FRISK - KNOWLEDGE OF SUSPECT'S PRIOR CRIM BEHAV\n",
    "    - ac_stsnd: ADDITIONAL CIRCUMSTANCES - SIGHTS OR SOUNDS OF CRIMINAL ACTIVITY\n",
    "    - ac_other: ADDITIONAL CIRCUMSTANCES - OTHER\n",
    "    - sb_hdobj: BASIS OF SEARCH - HARD OBJECT\n",
    "    - sb_outln: BASIS OF SEARCH - OUTLINE OF WEAPON\n",
    "    - sb_admis: BASIS OF SEARCH - ADMISSION BY SUSPECT\n",
    "    - sb_other: BASIS OF SEARCH - OTHER\n",
    "    - repcmd: REPORTING OFFICER'S COMMAND (1 TO 999)\n",
    "    - revcmd: REVIEWING OFFICER'S COMMAND (1 TO 999)\n",
    "    - rf_furt: REASON FOR FRISK - FURTIVE MOVEMENTS\n",
    "    - rf_bulg: REASON FOR FRISK - SUSPICIOUS BULGE\n",
    "    - offverb: VERBAL STATEMENT PROVIDED BY OFFICER (IF NOT IN UNIFORM)\n",
    "    - offshld: SHIELD PROVIDED BY OFFICER (IF NOT IN UNIFORM)\n",
    "    - forceuse: REASON FORCE USED\n",
    "    - dob: SUSPECT'S DATE OF BIRTH (CCYY-MM-DD)\n",
    "    - ht_feet: SUSPECT'S HEIGHT (FEET)\n",
    "    - ht_inch: SUSPECT'S HEIGHT (INCHES)\n",
    "    - weight: SUSPECT'S WEIGHT\n",
    "    - haircolr: SUSPECT'S HAIRCOLOR\n",
    "    - eyecolor: SUSPECT'S EYE COLOR\n",
    "    - build: SUSPECT'S BUILD\n",
    "    - othfeatr: SUSPECT'S OTHER FEATURES (SCARS, TATOOS ETC.)\n",
    "    - addrtyp: LOCATION OF STOP ADDRESS TYPE\n",
    "    - rescode: LOCATION OF STOP RESIDENT CODE\n",
    "    - premtype: LOCATION OF STOP PREMISE TYPE\n",
    "    - premname: LOCATION OF STOP PREMISE NAME\n",
    "    - addrnum: LOCATION OF STOP ADDRESS NUMBER\n",
    "    - stname: LOCATION OF STOP STREET NAME\n",
    "    - stinter: LOCATION OF STOP INTERSECTION\n",
    "    - crossst: LOCATION OF STOP CROSS STREET\n",
    "    - aptnum: LOCATION OF STOP APT NUMBER\n",
    "    - state: LOCATION OF STOP STATE\n",
    "    - zip: LOCATION OF STOP ZIP CODE\n",
    "    - addrpct: LOCATION OF STOP ADDRESS PRECINCT\n",
    "    - sector: LOCATION OF STOP SECTOR\n",
    "    - beat: LOCATION OF STOP BEAT\n",
    "    - post: LOCATION OF STOP POST\n",
    "    - xcoord: LOCATION OF STOP X COORD\n",
    "    - ycoord: LOCATION OF STOP Y COORD\n",
    "    - dettypCM: DETAILS TYPES CODE\n",
    "    - lineCM: COUNT >1 ADDITIONAL DETAILS\n",
    "    \n",
    "- After dropping these columns, the remaining columns are\n",
    "    - year: YEAR OF STOP (CCYY)\n",
    "    - datestop: DATE OF STOP (MM-DD-YYYY)\n",
    "    - timestop: TIME OF STOP (HH:MM)\n",
    "    - perobs: PERIOD OF OBSERVATION (MMM)\n",
    "    - crimsusp: CRIME SUSPECTED\n",
    "    - perstop: PERIOD OF STOP (MMM)\n",
    "    - explnstp: DID OFFICER EXPLAIN REASON FOR STOP?\n",
    "    - othpers: WERE OTHER PERSONS STOPPED, QUESTIONED OR FRISKED?\n",
    "    - arstmade: WAS AN ARREST MADE?\n",
    "    - arstoffn: OFFENSE SUSPECT ARRESTED FOR\n",
    "    - sumissue: WAS A SUMMONS ISSUED?\n",
    "    - frisked: WAS SUSPECT FRISKED?\n",
    "    - searched: WAS SUSPECT SEARCHED?\n",
    "    - contrabn: WAS CONTRABAND FOUND ON SUSPECT?\n",
    "    - pistol: WAS A PISTOL FOUND ON SUSPECT?\n",
    "    - riflshot: WAS A RIFLE FOUND ON SUSPECT?\n",
    "    - asltweap: WAS AN ASSAULT WEAPON FOUND ON SUSPECT?\n",
    "    - knifcuti: WAS A KNIFE OR CUTTING INSTRUMENT FOUND ON SUSPECT?\n",
    "    - machgun: WAS A MACHINE GUN FOUND ON SUSPECT?\n",
    "    - othrweap: WAS ANOTHER TYPE OF WEAPON FOUND ON SUSPECT\n",
    "    - pf_hands: PHYSICAL FORCE USED BY OFFICER - HANDS\n",
    "    - pf_wall: PHYSICAL FORCE USED BY OFFICER - SUSPECT AGAINST WALL\n",
    "    - pf_grnd: PHYSICAL FORCE USED BY OFFICER - SUSPECT ON GROUND\n",
    "    - pf_drwep: PHYSICAL FORCE USED BY OFFICER - WEAPON DRAWN\n",
    "    - pf_ptwep: PHYSICAL FORCE USED BY OFFICER - WEAPON POINTED\n",
    "    - pf_baton: PHYSICAL FORCE USED BY OFFICER - BATON\n",
    "    - pf_hcuff: PHYSICAL FORCE USED BY OFFICER - HANDCUFFS\n",
    "    - pf_pepsp: PHYSICAL FORCE USED BY OFFICER - PEPPER SPRAY\n",
    "    - pf_other: PHYSICAL FORCE USED BY OFFICER - OTHER\n",
    "    - sex: SUSPECT'S SEX\n",
    "    - race: SUSPECT'S RACE\n",
    "    - age: SUSPECT'S AGE\n",
    "    - city: LOCATION OF STOP CITY\n",
    "    - detailCM: CRIME CODE DESCRIPTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. The next step is to rename the remaining columns with a standardized naming method that will be applied to both the 2011 and 2019 Stop and Frisk datasets. Here is the list of renamed columns:\n",
    "    - datestop -> date\n",
    "    - timestop -> time\n",
    "    - perobs -> obs_duration\n",
    "    - perstop -> stop_duration\n",
    "    - explnstp -> off_explain\n",
    "    - othpers -> other_stop\n",
    "    - contrabn -> contraband\n",
    "    - knifcuti -> knife\n",
    "    - othrweap -> other_weapon\n",
    "    - city -> boro\n",
    "    - detailcm -> crime_sus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The Stop, Question, and Frisk dataset has multiple columns dedicated to whether weapons were found on the subject. These columns include:\n",
    "    - pistol: WAS A PISTOL FOUND ON SUSPECT?\n",
    "    - riflshot: WAS A RIFLE FOUND ON SUSPECT?\n",
    "    - asltweap: WAS AN ASSAULT WEAPON FOUND ON SUSPECT?\n",
    "    - knifcuti: WAS A KNIFE OR CUTTING INSTRUMENT FOUND ON SUSPECT?\n",
    "    - machgun: WAS A MACHINE GUN FOUND ON SUSPECT?\n",
    "    - othrweap: WAS ANOTHER TYPE OF WEAPON FOUND ON SUSPECT\n",
    "\n",
    "The methods in which weapons data was recorded in the 2011 dataset differ from the methods used in the 2019 dataset. To resolve these differentiations, the weapons data is condensed into 3 columns: firearm, knife, and other_weapon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. When an officer uses physical force on a suspect, there are many categories that the physical force can be classified into: hands, suspect against wall, suspect on ground, weapon drawn, weapon pointed, baton, handcuffs, pepper spray, and other. We are mainly interested in whether or not a weapon was drawn and / or pointed. In order to condense the physical force data, we made a general physical force column that records whether or not any physical force was used during the encounter, but will be keeping a separate column specifying if a weapon was drawn and / or pointed during the encounter. These two columns are named phys_force, and pt_draw_force."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect of the dataset that we have to account for is the age column. In the age column, it seems that if an officer did not know the age of the suspect, the age was coded as a number far above what is humanly possible - anywhere from ~145 to 999. Part of this error could also be due to typos. For example, when entering the police report into the dataset, an officer could have added an extra 0 to '15' to make it '150'. We made the executive decision to drop any rows that had an age listed above 110, because it is not realistic to analyze this when we do not know the real age of the suspect. In addition, we will be using age data a lot in out analysis, so these errors would disrupt later analysis.\n",
    "\n",
    "There is also prevalence of instances with a remarkably low age, such as 0 or 1. These could be real ages of suspects who were stopped - perhaps they were infants accompanyed by an adult and both were stopped and searched - but we also considered that these ages were due to typos as well. Because of this, we made another executive decision to drop any instances of people below the age of 13.\n",
    "\n",
    "5. After dropping the low ages and high ages, the subset we will be looking at will be inclusive of teenagers (ages 13-17) and adults (ages 18-110)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Boro data will be an important part of our analysis, so we checked for any instances where Boro contained an empty value. For all of the instances where Boro was empty, we dropped the row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. The Race data in 2011 was coded using codes that differed from the 2019 dataset. To resolve this, we recoded the race data in 2011 with the full names of each racial category. A similar recoding step was done to the Sex data as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. The Observation Duration data and Stop Duration data seemed to have encoded 'dummy' values of 999 when the Stop Duration and/or Observation Duration was unknown. There is no documentation of this practice in the dataset's specifications, so there was an element of inference our group had to make to come to this conclusion. However, it is likely that 999 is code for unknown because there are a handful of 999 values in these columns, and it is the max value present in both columns. If we were to keep these 999 values, they would definitely cause issues with analysis later because they are extreme outliers in the datasets. For these reasons, we dropped all instances of Observation Duration and Stop Duration that contained a value of 999."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. All columns whose data can be represented by True/False values were recoded as boolean values. The columns that were coverted to boolean data include\n",
    "    - off_explain\n",
    "    - other_stop\n",
    "    - arstmade\n",
    "    - sumissue\n",
    "    - frisked\n",
    "    - searched\n",
    "    - contraband\n",
    "    - knife\n",
    "    - other_weapon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. The last step is to convert the date and time columns into datetime objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the Stop, Question, and Frisk (2011) dataset cleaning has been completed. A 10% random sample was drawn from this cleaned dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset #2: Stop, Question, and Frisk (2019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Stop, Question, and Frisk (2019) dataset went through similar data cleaning steps as the 2011 dataset. Steps 1-6 were relatively similar as listed above, except that the Weapons and Physical Force data was stored using different methods than the 2011 dataset. For example, the 2019 dataset only contained one column indicating whether a suspect carried a firearm on them, labeled firearm. The 2011 dataset, on the other hand, differentiated between pistols, rifles, assault weapons, and machine guns. After cleaning these columns, though, both datasets' data is recorded using the same parameters so the data can be compared for analysis.\n",
    "\n",
    "Step 7 was unnecessary for the 2019 dataset because race data was already coded using the racial categories' full names. The only necessary modification to the race data was to recode races from '(null)' to 'UNKNOWN'.\n",
    "\n",
    "Step 8 was also unnecessary because the 2019 dataset did not contain any 999 values for Observation Duration or Stop Duration.\n",
    "\n",
    "Steps 9 and 10 were applied to the 2019 dataset, completing the data cleaning for the Stop, Question, and Frisk (2019) dataset. A 25% random sample was drawn from this cleaned dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset #3: New York City Crime Data (2009-2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset #4: Aggregated Stop, Question, and Frisk Instances by Year (2009-2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
